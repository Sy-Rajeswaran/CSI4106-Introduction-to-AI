{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSI4106-RL_Fall2021.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KsCQnroFWbpA"},"source":["# Notebook 10 - Reinforcement Learning / Self-Driving Cab"]},{"cell_type":"markdown","metadata":{"id":"jS9f1M7hWblx"},"source":["CSI4106 Artificial Intelligence   \n","Fall 2021  \n","Version 1 (2020) prepared by Julian Templeton and Caroline Barrière.  Version 2 (2021) adapted by Caroline Barrière."]},{"cell_type":"markdown","metadata":{"id":"jWknU1E2Wbi8"},"source":["***INTRODUCTION***:  \n","In this notebook we will be exploring the use of Reinforcment Learning to help allow an agent solve a specific task in an environment provided by [OpenAI's Gym library](https://gym.openai.com/). OpenAI's Gym provides many different experiments to use. These range from balancing acts to self driving cars to playing a simple Atari game. Unfortunately not every option available to us can be easily worked with. Many can take hours of training to start seeing some exciting results. Each of these experiments use agents that can be trained by Reinforcment Learning to master how to perform the specified task. The methods used can range from the simple use of Q-Learning to the more complex use of one or more Deep Learning models that work in conjunction with Reinforcement Learning techniques. \n","\n","Within this notebook we will be exploring a scenario in which an autonomous taxi located on a grid must pickup a passenger located in one of four positions and drop the passenger off in one of three other positions.    \n","\n","To familiarize yourself with the **Self-Driving Cab problem** tackled in this notebook, please go to the site https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ and read:  \\\n","\n","*   section 1 (rewards)\n","*   section 2 (state space) which will make you understand why there are 500 possible states\n","*   section 3 (action space) which describes the possible actions.  \n","\n","Throughout the notebook we will be working with a random baseline approach and a Q-Learning-based approach. This will provide insight into how Q-Learning can be applied to problems and how an agent can use Reinforcment Learning to solve problems in an environment.  Comparison with the baseline approach will show the potential of Q-Learning.  \n","\n","**When submitting this notebook, ensure that you do NOT reset the outputs from running the code (plus remember to save the notebook with ctrl+s).**      \n","\n","**In order to keep the installation easy, you will be once again running this notebook in Google Colab, NOT on your local machine.**    "]},{"cell_type":"markdown","metadata":{"id":"7iCMGO9VWbgM"},"source":["***HOMEWORK***:  \n","Go through the notebook by running each cell, one at a time.  \n","Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, sign the notebook (at the end of the notebook), rename it to *StudentNumber-LastName-Notebook10.ipynb* and submit it.  \n","\n","*The notebook will be marked on 30.  \n","Each **(TO DO)** has a number of points associated with it.*\n","***"]},{"cell_type":"markdown","metadata":{"id":"uIhGNTB-Wbbs"},"source":["**1.0 - Setting up the Taxi Game**   \n","\n","To begin the notebook, we need to set up and explore the Open Gym autonomous taxi environment.  \n","\n","The autonomous taxi will pick up and dropoff a passenger within a small grid. To really understand the environment in which the autonomous taxi (the agent) evolves, make sure you read the 3 sections about rewards, state space and action space from the tutorial site, as mentioned in the introduction of this notebook.\n","\n","The code used throughout the notebook comes from [this example](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/) and has been modified accordingly."]},{"cell_type":"markdown","metadata":{"id":"kEQK-Xep0MrF"},"source":["To start, we will install some of the packages that we will need to run the progam."]},{"cell_type":"code","metadata":{"id":"BPYIG4d6Ztzk"},"source":["# Install the necessary libraries\n","!pip install cmake 'gym[atari]' scipy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0vOVuqHA8SM"},"source":["# Import the necessary libraries\n","import random\n","import gym\n","import numpy as np\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z3nhPCUHb2cO"},"source":["With all of the libraries installed, we will now make use of the Taxi program provided by Gym. Below we will import Gym, load the program as the active environment, and render an image representing the current state of the program.   \n","\n","From the image seen below, there are four different key locations in the environment, represented by *R*, *G*, *Y*, and *B*. The letter that is *bolded in blue* represents where the current passenger needs to get picked up and the letter *bolded in purple* represents where the passenger wants to dropped off. The *yellow block* represents the cell which the taxi cab is currently located at. Therefore, the taxi cab must first pick up the passenger and drop them off at the dropoff location. When a passenger is in the taxi, the yellow block turns to a *green block* until the passenger is dropped off."]},{"cell_type":"code","metadata":{"id":"f2iMY2MbaFSk"},"source":["# Load the environment\n","env = gym.make(\"Taxi-v3\").env\n","# Render the current state of the program\n","env.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6oq2BvrwdPh-"},"source":["Next we will reset the state of the environment and re-render the current state. We also print the total number of actions available to our agent (defined as the *Action Space*) and the *State Space* which represents the state of the program (where is the cab, the passenger, pickup location and dropoff location)."]},{"cell_type":"code","metadata":{"id":"JlLevJ7KaHYc"},"source":["env.reset() # reset environment to a new, random state\n","env.render()\n","\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LGp8quCqeGjU"},"source":["We want our agent to learn which action to take given a specific state. This state depends on where the taxi cab is located in relation to the passenger location and drop off location. The six possible actions that the taxi can take at a given time step are:    \n","\n","Action = 0: Head south    \n","Action = 1: Head north    \n","Action = 2: Head east    \n","Action = 3: Head west    \n","Action = 4: Pickup     \n","Action = 5: Dropoff    \n","\n","Below is an example of setting the state to a specific encoding and rendering that state."]},{"cell_type":"code","metadata":{"id":"x7QM71s0aded"},"source":["# The encoding below represents: (taxi row, taxi column, passenger location, destination index)\n","state = env.encode(3, 1, 2, 0) \n","print(\"State:\", state)\n","\n","env.s = state\n","env.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kCSHJMfb2Hll"},"source":["The following example showcases how a state with the passenger within the taxi can be set."]},{"cell_type":"code","metadata":{"id":"xPshGWDomKfc"},"source":["# The encoding below represents: (taxi row, taxi column, passenger location, destination index)\n","state = env.encode(0, 1, 4, 0) \n","print(\"State:\", state)\n","\n","env.s = state\n","env.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i3b__QWRTRB3"},"source":["**(TO DO) Q1 - 4 marks**    \n","Now that we have seen how to set a state via an encoding, you will need to set the state to match two different descriptions below and render them.   "]},{"cell_type":"markdown","metadata":{"id":"0YKbEIP2WAkb"},"source":["**(TO DO) Q1 (a) - 2 marks**    \n","Set the passenger to be at position G, with the passenger wanting to be dropped off at position R, and the taxi positioned at a random point on the grid (the selected position of the taxi must be selected randomly). After setting the position, render the state.   "]},{"cell_type":"code","metadata":{"id":"ugCARMZY3qoe"},"source":["# ANSWER Q1(a) \n","# remember to use random coordinates within the grid for the taxi...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pl8bVtwWIdE"},"source":["**(TO DO) Q1 (b) - 2 marks**    \n","Set the passenger to be in the taxi (at any position without a letter on it) and set the passenger dropoff point to be position B. After setting the position, render the state."]},{"cell_type":"code","metadata":{"id":"U0rpk0D830DO"},"source":["# ANSWER Q1(b)\n","# ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zTyOS_Hbi3Rf"},"source":["For every action that the taxi can take, we have a list representing the key information with respect to what will happen when an action is performed. After performing an action, the agent will receive a reward or a penalty from the environment which will then be in a different state.\n","\n","Below we display a dictionary (very similar to the Reward Table we discussed in the video lecture) that contains all possible actions along with the following information within the corresponding tuples:     \n","\n","(     \n","  The probability of taking that action,     \n","  The resulting state after taking that action,    \n","  The reward for taking that action,    \n","  Whether or not the program will end when performing the action   \n",")      \n","\n","Example tuple: (1.0, 328, -1, False)"]},{"cell_type":"code","metadata":{"id":"a6Ga3Y6yagfE"},"source":["env.P[328]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_57Z2adUzAJH"},"source":["Although not displayed by the code above, if the taxi is holding the passenger and is over the dropoff point, the reward for the dropoff action is 20."]},{"cell_type":"markdown","metadata":{"id":"Iiy-KNusvBDJ"},"source":["**2.0 - Baseline Approach to the Taxi Game**   \n","\n","To start, we will perform the simulation of the taxi cab scenario with a baseline approach that does not use Q-Learning. This approach will simply work by selecting a random available action at each time step, regardless of the current state. We will also prepare a method of playing through all frames within an episode to view how the agent controls the taxi in the scenario."]},{"cell_type":"code","metadata":{"id":"dKnHAA5SaitM"},"source":["def run_single_simulation_baseline(env, state, disable_prints=False):\n","    '''\n","    Given the environment and a specific state, randomly select an action for the taxi\n","    to perform until the goal is completed.\n","    '''\n","    if not disable_prints:\n","        print(\"Testing for simulation: {}\".format(state))\n","    # Set the state of the environment\n","    env.s = state\n","    # Used to hold all information for a single time step (including the image data)\n","    frames = []\n","    # Used to determine when the simulation has been completed\n","    done = False\n","    # Determines the number of times steps that the application has been run for\n","    time_steps = 0\n","    # The total values used to determine how many times the agent mistakenly\n","    # picks up no one or attempts to dropoff no passenger or attempts to\n","    # dropoff a passenger in the wrong position.\n","    penalties, reward = 0, 0\n","    # Run until the passenger has been picked up and dropped off in the target location\n","    while not done:\n","        # Perform a random action from the set of available actions in the environment\n","        action = env.action_space.sample()\n","        # From performing the action, retrieve the new state, the reward from taking the action,\n","        # whether the simulation is complete, and other information from performing the action.\n","        state, reward, done, info = env.step(action)\n","        # If an incorrect dropoff or pickup is performed, increment the penalty count\n","        if reward == -10:\n","            penalties += 1\n","        # Put each rendered frame into dict to use for animating the process and\n","        # tracking the details over the run\n","        frames.append({\n","            'frame': env.render(mode='ansi'),\n","            'state': state,\n","            'action': action,\n","            'reward': reward\n","            }\n","        )\n","        # Increment the time step count\n","        time_steps += 1\n","    # State the total number of steps taken and the total penalties that have occured.\n","    if not disable_prints:\n","        print(\"Timesteps taken: {}\".format(time_steps))\n","        print(\"Penalties incurred: {}\".format(penalties))\n","    # Return the frame data, the total penalties, and the total time steps\n","    return frames, penalties, time_steps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fk5lkc3Q5XLv"},"source":["With the baseline approach defined, we will run a test with this approach to see how long it takes an agent using this approach to find a solution for simulation 328 and how many major penalties the agent receives."]},{"cell_type":"code","metadata":{"id":"zTLKSCRq4bEG"},"source":["state = 328\n","# Run a test and collect all frames from the run\n","frames, _, _ = run_single_simulation_baseline(env, state)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayN1up479Fl1"},"source":["After performing a simulation and retrieving the results, we can use the frames obtained from the simulation and pass it to the *print_frames* function below to display an animation containing all frames along with the information within that frame at each timestep.    \n","\n","For the first episode that you view, it is recommended to run through the entire process at a slower speed (such as 0.3 or 0.5 for the sleepTime parameter). However you are free to increase the speed of the process later on."]},{"cell_type":"code","metadata":{"id":"Bt-sMubValJt"},"source":["from IPython.display import clear_output\n","from time import sleep\n","\n","def print_frames(frames, sleepTime=0.3):\n","    '''\n","    For each frame, show the frame and display the timestep it occurred at,\n","    the number of the active state, the action selected, adn the corresponding reward.\n","    '''\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        # Can adjust speed here\n","        sleep(sleepTime)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsgBOQI3MpG-"},"source":["# Print the frames from the episode\n","print_frames(frames, 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRCoIPbi7OCv"},"source":["**(TO DO) Q2 - 2 marks**   \n","Let's look at a few simulations to see how the random approach behaves.  To do so, we'll start from the states you've defined in Q1."]},{"cell_type":"markdown","metadata":{"id":"Y-N9fP_Ydvot"},"source":["**(TO DO) Q2 (a) - 1 mark**   \n","Using the state defined from Q1 (a), retrieve the corresponding frames obtained from using the baseline approach defined above. Then display those frames."]},{"cell_type":"code","metadata":{"id":"_4IqQE634Lm-"},"source":["# ANSWER Q2(a)\n","# Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3Dufixud28j"},"source":["**(TO DO) Q2 (b) - 1 mark**   \n","Using the state defined from Q1 (b), retrieve the corresponding frames obtained from using the baseline approach defined above. Then display those frames."]},{"cell_type":"code","metadata":{"id":"zfEO2-zZ4To9"},"source":["# ANSWER Q2(b)\n","# Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cwgxcH_Ojfud"},"source":["With the ability to simulate single runs of an episode with the Baseline approach, we will now define a function that we will use to evaluate the general performance of the baseline model when averaged over many episodes. The *evaluate_agent_baseline* function below accepts as input the total number of randomly selected episodes to run along with the environment, runs the random episodes, displays the average amount of timesteps taken per episode along with the average penalties incurred, and returns the frame data.     \n","\n"]},{"cell_type":"code","metadata":{"id":"7GLAT5_mjgEu"},"source":["def evaluate_agent_baseline(episodes, env):\n","    '''\n","    Given a number of episodes and an environment, run the specified\n","    number of episodes, where each run begins with a random state, display the\n","    naverage timesteps per episode and the average penalties per episode, and output\n","    the frames to be displayed.\n","    '''\n","    total_time_steps, total_penalties = 0, 0\n","    frames = []\n","    # Run through the total number of episodes\n","    for _ in range(episodes):\n","        # Get a random state\n","        state = env.reset()\n","        # Run the simulation, obtaining the results\n","        frame_data, penalties, time_steps = run_single_simulation_baseline(env, state, True)\n","        # Update the tracked data over all simulations\n","        total_penalties += penalties\n","        total_time_steps += time_steps\n","        frames = frames + frame_data\n","    print(f\"Results after {episodes} episodes:\")\n","    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n","    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","    return frames"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHkepDd2C3im"},"source":["**(TO DO) Q3 - 5 marks**    \n","Let's evaluate the baseline approach.  We must run a certain number of episodes to average the results."]},{"cell_type":"markdown","metadata":{"id":"w3Tnw6bBxMXv"},"source":["**(TO DO) Q3 (a) - 1 mark**    \n","Use the *evaluate_agent_baseline* function defined above to run through 100 random episodes for the environment.   \n","\n","***If the evaluate_agent_baseline function ever seems to be running for far too long (several minutes, not just one), stop the run by clicking the button at the top-left of the code cell being executed and run it again.***"]},{"cell_type":"code","metadata":{"id":"N1BQOaSu4awd"},"source":["# ANSWER Q3(a)\n","# ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dClJX6AWxZM5"},"source":["**(TO DO) Q3 (b) - 2 marks**    \n","From the output seen from Q3 (a), how did the Baseline approach do and why do you think that it performed well or poorly? Explain with respect to the average timesteps per episode and the average penalties per episode.     "]},{"cell_type":"markdown","metadata":{"id":"pWVDzBDc3djW"},"source":["**ANSWER Q3(b)**   \n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"7Wr3F6IExrQQ"},"source":["**(TO DO) Q3 (c) - 2 marks**    \n","Without suggesting to use a Reinforcment Learning approach (as we would do next), suggest 2 ideas that could be included in a Baseline+ approach to allow it to perform slightly better?"]},{"cell_type":"markdown","metadata":{"id":"JQRkm9-I4j2G"},"source":["**ANSWER Q3(c)**\n","\n","...\n"]},{"cell_type":"markdown","metadata":{"id":"4GfrTVl3AuhD"},"source":["**3.0 - Training an Agent with Q-Learning to play the Taxi Game**   \n","\n","Now that we have seen simulations of the autonomous taxi using a random strategy, we will use Q-Learning to try applying a Reinforcement Learning approach to the problem and have the autonomous taxi learn a better strategy.\n","\n","To start the process, we will create a table of Q values for each action-state possibility (initializing it as zero). The agent will update this table when training and ATTENTION will need to ***reinitialize*** the table whenever the agent wants to restart its training."]},{"cell_type":"code","metadata":{"id":"IDTbzsaEanoU"},"source":["# Initialize the table of Q values for the state-action pairs\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mph6_GdSGy2F"},"source":["With the Q-table initialized, we will now define the training function that adjusts the Q values within *q_table*. The training process consists of running through a number of random simulations and updating the Q values for each state via Q-Learning.    \n","\n","There are a number of hyperparameters used by the training function:    \n","\n","- *alpha*: Learning parameter \n","- *gamma*: The long term reward discount parameter.    \n","- *epsilon*: Exploitation/Exploration parameter \n","- *num_simulations*: Represents how many episodes should be generated for the autonomous taxi update the Q-values.\n","\n","Thus, by running through this learning algorithm, an agent can learn which Q-values to use when later put in a test situation."]},{"cell_type":"code","metadata":{"id":"6kbLhT-Ua0Cc"},"source":["def train_agent(alpha, gamma, epsilon, num_simulations):\n","    '''\n","    Trains an agent by updating its Q values for a total of num_simulations\n","    episodes with the alpha, gamma, and epsilon hyperparameters. \n","    '''\n","    # For plotting metrics\n","    all_time_steps = []\n","    all_penalties = []\n","    # Generate the specified number of episodes\n","    for i in range(1, num_simulations + 1):\n","        # Generate a new state by resetting it\n","        state = env.reset()\n","        # Variables tracked (time steps, total penalties, the reward value)\n","        time_steps, penalties, reward, = 0, 0, 0\n","        done = False\n","        # Run the simulation \n","        while not done:\n","            # Select a random action is the randomly selected number from a\n","            # uniform distribution is less than epsilon\n","            if random.uniform(0, 1) < epsilon:\n","                action = env.action_space.sample() # Explore action space\n","            # Otherwise use the currently learned Q values\n","            else:\n","                action = np.argmax(q_table[state]) # Exploit learned values\n","            # Retrieve the relevant information after performing the action\n","            next_state, reward, done, info = env.step(action) \n","            # Retrieve the old Q value and the maximum Q value from the next state\n","            old_value = q_table[state, action]\n","            next_max = np.max(q_table[next_state])\n","            # Update the current Q value\n","            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","            q_table[state, action] = new_value\n","            # Track anytime an incorrect dropoff or pickup is made\n","            if reward == -10:\n","                penalties += 1\n","            # Proceed to the next state and time step\n","            state = next_state\n","            time_steps += 1\n","        # Display progress for each 100 episodes\n","        if i % 100 == 0:\n","            clear_output(wait=True)\n","            print(f\"Episode: {i}\")\n","    print(\"Training finished.\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLoNggkqQu2L"},"source":["We now use the training function with a set of hyperparameters to train the agent with Q-Learning to potentially improve performance over time.  The number of episodes is set to 100000, so the learning will take a bit of time."]},{"cell_type":"code","metadata":{"id":"_tkJvaTwPjfB"},"source":["# Hyperparameters\n","alpha = 0.1\n","gamma = 0.5\n","epsilon = 0.1\n","num_simulations = 100000\n","# Train the agent\n","train_agent(alpha, gamma, epsilon, num_simulations)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmzQceEvRyT3"},"source":["After the training, we can look at the Q-values that have been obtained in our state-action table for a specific state. Below we see that each Q-value for the six possible actions available for state 328 have been updated accordingly."]},{"cell_type":"markdown","metadata":{"id":"nvXOzHtVAar6"},"source":["**(TO DO) Q4 - 2 marks**     \n","Below we print the Q-values that are available for the six actions at state 328 and we render that state to view it. Based on the available Q-values (assuming we are in exploitation mode), which action would be the next to be selected (or if there are ties, list all possible actions that would be considered)? Do any of the actions that contain larger Q values seem problematic if they were selected? Why or why not?"]},{"cell_type":"code","metadata":{"id":"-KP5FMBQa1el"},"source":["print(q_table[328])\n","env.s = 328\n","env.render()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0ms4hY68Fpg"},"source":["**ANSWER Q4**\n","\n","...\n"]},{"cell_type":"markdown","metadata":{"id":"ncwGTV5PSHZx"},"source":["With the training complete, we can now evaluate the Q-Learning approach in a similar method that we used to evaluate the Baseline approach. By passing the number of episodes to test for and the environment, we generate that number of random episodes and average the results obtained from running the Q-Learning approach to complete the episodes. Unlike the training, it is important to note that the hyperparameters that were used for learning are not used here. The agent simply uses the maximum Q-value at each step to determine which action to take at a given time step. This means we use a greedy policy (not even an epsilon-greedy one) which always trust the maximum Q-value to decide on its action.  It's a full exploitation mode (not exploration/exploitation).\n","\n"]},{"cell_type":"code","metadata":{"id":"CuitaI8Ra2u8"},"source":["def evaluate_agent_QL(episodes, env):\n","    '''\n","    Given a number to specify how many random states to run and the environment to use,\n","    display the averaged metrics obtained from the tests and return the frames obtained from the tests.\n","    '''\n","    total_time_steps, total_penalties = 0, 0\n","    frames = []\n","    for _ in range(episodes):\n","        # Generate a random state to use\n","        state = env.reset()\n","        # The information collected throughout the run\n","        time_steps, penalties, reward = 0, 0, 0\n","        # Determines when the episode is complete\n","        done = False\n","        # Run through the episode until complete\n","        while not done:\n","            # Select the action containing the maximum Q value\n","            action = np.argmax(q_table[state])\n","            # Run that action and retrieve the reward and other details\n","            state, reward, done, info = env.step(action)\n","\n","            # Put each rendered frame into dict for animation\n","            frames.append({\n","                'frame': env.render(mode='ansi'),\n","                'state': state,\n","                'action': action,\n","                'reward': reward\n","                }\n","            )\n","            # Specify whether the agent incorrectly chose to pick up or dropoff a passenger\n","            if reward == -10:\n","                penalties += 1\n","            # Increment the current time step\n","            time_steps += 1\n","        # Track the totals\n","        total_penalties += penalties\n","        total_time_steps += time_steps\n","    # Display the performance over the tests\n","    print(f\"Results after {episodes} episodes:\")\n","    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n","    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","    # Return the frames to allow a user to view the runs\n","    return frames"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WEQNpWoSF822"},"source":["**(TO DO) Q5 - 3 marks**     \n"," There is a random aspect in Q-learning, so we need to run for a certain number of episodes to provide an average result. Let's set that number high enough for the average to be significant. Let's see how Q-learning does compared to the baseline approach.\n","\n"," ***ATTENTION: If the evaluate_agent_QL function ever seems to be running for far too long (30s or more), stop the run by clicking the button at the top-left of the code cell being executed and run it again. This occurs because the training was insufficient at setting valid Q values and resulted in a dead-end for a specific state.***"]},{"cell_type":"markdown","metadata":{"id":"-TzyRekwGolK"},"source":["**(TO DO) Q5 (a) - 1 mark**     \n","Run the *evaluate_agent_QL* for 500 episodes to retrieve the average number of time steps and the average penalty after training.     "]},{"cell_type":"code","metadata":{"id":"ATGdih3m-TE_"},"source":["# ANSWER Q5(a)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_Os_s3UGxK2"},"source":["**(TO DO) Q5 (b) - 2 marks**     \n","Given your results from Q5 (a), how do the observed results from the tests compare to the tests from the Baseline model in Q3 (a)? Specifically, which policy performs better with respect to the average number of penalties throughout the tests and which strategy is able to take passengers from start to destination more rapidly (on average).     "]},{"cell_type":"markdown","metadata":{"id":"hxE9w0Zc-Ve9"},"source":["**ANSWER Q5(b)**\n","\n","...\n"]},{"cell_type":"markdown","metadata":{"id":"woAhj7vqaD5q"},"source":["**4.0 - Testing Different Hyperparameters**   \n","\n","Now we will try retraining the agent using different set ups for the hyperparameters. This will allow you to explore their impact on the Q-Learning as well as understand their purpose during the training.     "]},{"cell_type":"markdown","metadata":{"id":"WBamDNUIZ0Iy"},"source":["**(TO DO) Q6 - 12 marks**     \n","Below we explore variations for all four hyperparameters used by the Q-Learning approach to better understand their impact on the training. When answering the questions, ***be careful to correctly set the hyperparameters***.         \n","\n","As a note, below are the initial hyperparameter values used from section 3.0 of this notebook to use as reference:    \n","\n","*alpha* = 0.1   \n","*gamma* = 0.5   \n","*epsilon* = 0.1   \n","*num_simulations* = 100000 \n","\n","For all our evaluations, we will use 500 episodes.  Also, after each experiment, you will print the qtable value for state 328, so you can compare it with what was obtained in Q4 earlier.\n","\n","***ATTENTION (repeated from Q5): If the evaluate_agent_QL function ever seems to be running for far too long (30s or more), stop the run by clicking the button at the top-left of the code cell being executed and run it again. This occurs because the training was insufficient at setting valid Q values and resulted in a dead-end for a specific state.***"]},{"cell_type":"markdown","metadata":{"id":"UQxxvn0xaxrC"},"source":["**(TO DO) Q6 (a) - 1.5 marks**     \n","Retrain the agent by resetting the Q learning values and training for only **35000 episodes** (with the same alpha, gamma, and epsilon values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.  Print the *q_table* for state 328."]},{"cell_type":"code","metadata":{"id":"CgprtPOpJcUQ"},"source":["# ANSWER Q6(a)\n","# TODO: Reset q_table\n","q_table = ...\n","# TODO: Retrain with the specified hyperparameters\n","# Hyperparameters\n","alpha = ...\n","gamma = ...\n","epsilon = ...\n","num_simulations = ...\n","...\n","# TODO: Test for 500 episodes\n","...\n","# TODO: Print q_table for state 328\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kSCmM6VFdHu"},"source":["**(TO DO) Q6 (b) - 1.5 marks**      \n","Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **epsilon value of 0.8** (with the same alpha and gamma values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.  Print the *q_table* for state 328."]},{"cell_type":"code","metadata":{"id":"s55VwNKJJzn_"},"source":["# ANSWER Q6(b)\n","# TODO: Reset q_table\n","q_table = ...\n","# TODO: Retrain with the specified hyperparameters\n","# Hyperparameters\n","alpha = ...\n","gamma = ...\n","epsilon = ...\n","num_simulations = ...\n","...\n","# TODO: Test for 500 episodes\n","...\n","# TODO: Print q_table for state 328\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWcn_doxFm4H"},"source":["**(TO DO) Q6 (c) - 1.5 marks**      \n","Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **alpha value of 0.7** (with the same gamma and epsilon values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.    Print the *q_table* for state 328. "]},{"cell_type":"code","metadata":{"id":"yNwWugyUJ3VZ"},"source":["# ANSWER Q6(c)\n","# TODO: Reset q_table\n","q_table = ...\n","# TODO: Retrain with the specified hyperparameters\n","# Hyperparameters\n","alpha = ...\n","gamma = ...\n","epsilon = ...\n","num_simulations = ...\n","...\n","# TODO: Test for 500 episodes\n","...\n","# TODO: Print q_table for state 328\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YZwH8XogFreB"},"source":["**(TO DO) Q6 (d) - 1.5 marks**      \n","Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **gamma value of 0.15** (with the same alpha and epsilon values used in section 3.0 of this notebook). Then perform another test for 500 episodes with the environment.  Print the *q_table* for state 328.  "]},{"cell_type":"code","metadata":{"id":"QPS0kF_YJ51X"},"source":["# ANSWER Q6(d)\n","# TODO: Reset q_table\n","q_table = ...\n","# TODO: Retrain with the specified hyperparameters\n","# Hyperparameters\n","alpha = ...\n","gamma = ...\n","epsilon = ...\n","num_simulations = ...\n","...\n","# TODO: Test for 500 episodes\n","...\n","# TODO: Print q_table for state 328\n","..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ec4qnmyJbWYw"},"source":["**(TO DO) Q6 (e) - 6 marks**      \n","Using the results obtained from your tests in Q6 (a), (b), (c), and (d), along with the initial results found from Q5 to serve as the Q-Learning baseline to compare with, explain the impacts of modifying the number of episodes trained on (less vs more), the alpha value (lower vs higher), the gamma value (lower vs higher), and the epsilon value (lower vs higher).  Even if the difference in the comparisons are minor, state them.  \n","\n","Besides looking at the obtained results (average number of timesteps), discuss the impact on the q-values for the different actions in state 328. Did those change?\n"]},{"cell_type":"markdown","metadata":{"id":"6h6XkCsJKnGP"},"source":["**ANSWER Q6(e)**\n","\n","*Make sure you discuss each hyperparameter in relation to timesteps and qvalues.*\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"9Z7LljGSQ5bI"},"source":["**5.0 - Scaling up the environment**   \n","\n","As we've seen in the video lectures, and as you've experienced here, a limitation of Q-Learning is that Q-Values must be learned for all state/action pairs.  In our toy problem of the autonomous taxi, we worked with a small grid (5x5) and only 4 pick-up/drop-off locations, and we were already at 500 states, so with 6 actions, that meant 3000 state/action pairs, so 3000 q-values to learn.\n","\n","Imagine a larger grid of 100x100, and imagine all grid cells could be pick-up or drop-off cells, then Q-learning would not be a realistic approach and we would have to go to Deep Q-Learning, as we saw in the course videos.\n","\n","Implementing a Deep Q-Learning approach is beyond the scope of this notebook, but we can still think about the type of architecture it would require."]},{"cell_type":"markdown","metadata":{"id":"0FeWLLRhRYKX"},"source":["**(TO DO) Q7 - 2 marks**      \n","Given this new description of a larger grid, and a larger set of pick-up/drop-off locations, describe what a deep Q-learning neural network could look like.  What could be put at the input nodes and how many would be required?  What would we try to learn at the output nodes and how many output nodes would be required.?"]},{"cell_type":"markdown","metadata":{"id":"7VAzlr6URY5f"},"source":["**ANSWER Q7**\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"eiD_vGE6jMOu"},"source":["***SIGNATURE:***\n","My name is --------------------------.\n","My student number is -----------------.\n","I certify being the author of this assignment."]}]}